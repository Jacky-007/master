# CAT-Gen:通过受控对抗式文本生成提高NLP模型的鲁棒性

### 作者：

### 单位：

### 发表刊物：

### 发表年限：

摘要：NLP模型已被证明受到了随机性问题的影响，即在对输入的微小扰动下，模型的预测很容易被改变。在这项工作中，我们提出了一种可控对抗性文本生成（CAT- Gen）模型，给定一个输入文本，通过已知与任务标签无关的可控属性生成对抗性文本。例如，为了攻击一个在产品re-view上进行情感分类的模型，我们可以使用产品类别作为可控属性，它不应该改变评论的情感。在真实世界的NLP数据集上进行的实验表明，与许多现有的对抗性文本生成方法相比，我们的方法可以生成更多样、更丰富的对抗性文本。我们进一步使用我们生成的对抗性文本，通过对抗性训练来改进模型，并且我们证明了我们生成的攻击对模型再训练和不同的模型架构更加强大。

## 1.What is the problem the paper is trying to address?论文试图解决什么问题

论文



