{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "vocab = {}\n",
    "vocab['[PAD]'] = 0\n",
    "# 参数设置\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "seq_len = 25  # 每句话的长度\n",
    "embedding_size = 150  # word2vec的维度\n",
    "kernel_num = 75  # 每一种卷积核的个数\n",
    "Kernel_list = [2, 3, 4, 5,6]  # N-gram\n",
    "class_num = 9  # 分类的个数\n",
    "epoches = 100  #训练次数\n",
    "lr = 0.001  # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词向量分配（每一个词都对应一个数字）\n",
    "def pro_vocab(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        L = [e.strip() for e in f.readlines()]\n",
    "        for news in L:\n",
    "            news = news[2:]\n",
    "            for word in news:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_vocab('train.txt')\n",
    "pro_vocab('test.txt')\n",
    "\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2ids(text):  # 将数据转化成数字id 并多退少补\n",
    "    ids = [vocab[word] for word in text]\n",
    "    if len(ids) < seq_len:\n",
    "        ids += [0] * (seq_len - len(ids))\n",
    "        return ids\n",
    "    else:\n",
    "        return ids[:seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):  # 读取训练集和测试集\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        turples = [(int(sentence[0]), word2ids(sentence[2:].strip()))\n",
    "                   for sentence in f.readlines()]\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for turple in turples:\n",
    "        labels.append(turple[0])\n",
    "        texts.append(turple[1])\n",
    "    # 返回标签和经过预处理的文本\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = load_data('train.txt')\n",
    "test_x, test_y = load_data('test.txt')\n",
    "train_x = torch.LongTensor(train_x).to(device)\n",
    "train_y = torch.LongTensor(train_y).to(device)\n",
    "test_x = torch.LongTensor(test_x).to(device)\n",
    "test_y = torch.LongTensor(test_y).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "        V = vocab_size\n",
    "        E = embedding_size\n",
    "        Ci = 1  # 输入数据的通道数\n",
    "        Co = kernel_num  # 每一种卷积核的数目\n",
    "        Kl = Kernel_list  # N-gram\n",
    "        C = class_num  #输出的维度\n",
    "\n",
    "        self.embed = nn.Embedding(V, E)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(Ci, Co, (K, E)) for K in Kl])\n",
    "        self.fc = nn.Linear(len(Kl) * Co, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)  # (N, seq_len, E)\n",
    "\n",
    "        x = x.unsqueeze(1)  # (N, Ci = 1, seq_len, E)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3)\n",
    "             for conv in self.convs]  # [(N, Co, seq_len-ki+1), ...]*len(Ks)\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2)\n",
    "             for i in x]  # [(N, Co), ...]*len(Ks)\n",
    "        x = torch.cat(x, 1)\n",
    "        out = F.softmax(self.fc(x), dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextCNN().to(device)  #实例化模型\n",
    "criterion = nn.CrossEntropyLoss().to(device)  #定义损失函数，为交叉熵损失\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)  #使用随机梯度下降算法，学习率为lr\n",
    "\n",
    "for epoch in trange(epoches): #训练过程\n",
    "    pred = model(train_x)\n",
    "    loss = criterion(pred, train_y)\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▏                                                                        | 99/1500 [10:04<2:37:03,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6990107297897339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████▏                                                                  | 199/1500 [20:10<2:07:34,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5174521207809448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▎                                                             | 299/1500 [31:02<2:14:07,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5160112380981445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████████████▍                                                        | 399/1500 [40:52<1:43:23,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5155375003814697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████████████████▌                                                   | 499/1500 [51:45<1:44:02,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5148073434829712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████▉                                             | 599/1500 [1:01:25<1:21:40,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5146516561508179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████████████████████████████████▉                                        | 699/1500 [1:10:47<1:08:17,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5145577192306519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████▎                                     | 747/1500 [1:15:24<1:16:00,  6.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1339578afb10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred_y = model(test_x)\n",
    "L = pred_y.tolist()\n",
    "total = len(L)\n",
    "acc = 0\n",
    "for i, pred in enumerate(L):\n",
    "    max_index = pred.index(max(pred))\n",
    "    if max_index == test_y[i]:\n",
    "        acc += 1\n",
    "\n",
    "print(acc / total * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_4.txt', 'r', encoding='utf-8') as f:\n",
    "        turples = [(int(sentence[0]), word2ids(sentence[2:].strip()))\n",
    "                   for sentence in f.readlines()]\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for turple in turples:\n",
    "        labels.append(turple[0])\n",
    "        texts.append(turple[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>编号</th>\n",
       "      <th>内容</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3875</td>\n",
       "      <td>陈凯：致力盔甲工艺的文化传承</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2999</td>\n",
       "      <td>预付卡资金“打水漂”？ 消费者可以这样维权</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>356</td>\n",
       "      <td>一键生成100个商标名称 阿里向社会免费开放AI商标注册机器人</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>483</td>\n",
       "      <td>比利时新增1580例新冠肺炎病例 累计确诊24983例</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130</td>\n",
       "      <td>马来西亚星洲网：马来西亚中医药抗疫小组发布安全指南</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>2091</td>\n",
       "      <td>北京做好准备确保初高三如期开学 完善“一校一案”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>1083</td>\n",
       "      <td>龚琳娜在云南深山采风 感慨民间声乐一辈子也学不完</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>2014</td>\n",
       "      <td>北约陷入“走钢丝”窘境</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>2445</td>\n",
       "      <td>云南省陇川县章凤镇曼农村突发山火</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1316</td>\n",
       "      <td>泰国单日新增新冠确诊病例数持续下降</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       编号                               内容\n",
       "0    3875                   陈凯：致力盔甲工艺的文化传承\n",
       "1    2999            预付卡资金“打水漂”？ 消费者可以这样维权\n",
       "2     356  一键生成100个商标名称 阿里向社会免费开放AI商标注册机器人\n",
       "3     483      比利时新增1580例新冠肺炎病例 累计确诊24983例\n",
       "4     130        马来西亚星洲网：马来西亚中医药抗疫小组发布安全指南\n",
       "..    ...                              ...\n",
       "595  2091         北京做好准备确保初高三如期开学 完善“一校一案”\n",
       "596  1083         龚琳娜在云南深山采风 感慨民间声乐一辈子也学不完\n",
       "597  2014                      北约陷入“走钢丝”窘境\n",
       "598  2445                 云南省陇川县章凤镇曼农村突发山火\n",
       "599  1316                泰国单日新增新冠确诊病例数持续下降\n",
       "\n",
       "[600 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = pd.read_csv('test_4.txt',sep='\\t',names=[\"编号\", \"内容\"])\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
