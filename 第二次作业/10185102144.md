# 智能推荐系统第二次编程作业实验报告

## 																												-------------基于内容的推荐算法 + TF-IDF

[TOC] 

## 1. 算法简介

### **1.1 基于内容的推荐算法简介**

基于内容的推荐算法(Content-based Recommendations, CB)也是一种工业界应用比较广的一种推荐算法。它根据用户过去喜欢的item，为用户推荐和他过去喜欢的产品相似的item。例如，一个推荐饭店的系统可以依据某个用户之前喜欢很多的烤肉店而为他推荐烤肉店。 CB最早主要是应用在信息检索系统当中，所以很多信息检索及信息过滤里的方法都能用于CB中。

### 1.2 基于内容的推荐算法实现简单描述 

+ **Item Representation**：为每个item抽取出一些特征来表示此item

  > 这些属性通常可以分为两种：结构化的（structured）属性与非结构化的（unstructured）属性。所谓结构化的属性就是这个属性的意义比较明确，其取值限定在某个范围；而非结构化的属性往往其意义不太明确，取值也没什么限制，不好直接使用。

+ **Profile Learning**：利用一个用户过去喜欢（及不喜欢）的item的特征数据，来学习出此用户的喜好特征

+ **Recommendation Generation**：通过比较上一步得到的用户profile与候选item的特征，为此用户推荐一组相关性最大的item。

  > 本次作业要求输出的是用户点击某个新闻的可能性，可以理解为该界面的新闻是我们的候选推荐的item，而我们的推荐指数，就是用户点击的可能性

### 1.3 TF-IDF简介

* **为什么要用TF-IDF**：

  用户有历史的浏览记录，我们可以从这些用户历史浏览的新闻中”提取”能代表新闻主要内容的关键词，看哪些关键词出现的最多。比如可以有”手机“，”电脑游戏“，”发布会“等等关键词。

  或者，统计这些新闻所属的领域是哪些，比如国际政治、社会、民生、娱乐，找出用户看的新闻来源最多的几个领域。不过按这种方式判断用户兴趣容易太宽泛，哪怕是同一个领域下的新闻，可能也会差异很大。比如某用户可能喜欢A女星，而不喜欢B女星，而如果你只是认为该用户喜欢娱乐新闻，结果把B女星的新闻不停给用户推，那就肯定不好。而上述的关键词就可以比较好地规避这个问题。

  找到定义用户喜好的方法——关键词，那么我们自然而然就可以想到，能不能提取出两个新闻的关键词，然后对比看它们两的关键词是不是相同的呢？不过毕竟一个新闻可以有好几个关键词，要想全部一样，还是比较困难的。所以我们需要对两个新闻的关键词匹配程度做一个合理的量化。

  那么这时TFIDF算法就会变得非常可靠有效。

* **TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）是一种用于信息检索（information retrieval）与文本挖掘（text mining）的常用加权技术。**

  TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。

### 1.4 TF-IDF算法实现简单描述

* **TF是词频(Term Frequency)：**

  表示词条（关键字）在文本中出现的频率，计算公式如下：

  $ tf_{i,j} = \frac{n_{i,j}}{\sum _{k}n_{i,j}}$

   其中 $n_{i,j}$ 是该词在文件 $d_j$ 中出现的次数，分母则是文件$ d_j$ 中所有词汇出现的次数总和。

* **IDF是逆向文件频率(Inverse Document Frequency)：**

  某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目**，**再将得到的商取对数得到。

  如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。计算公式如下：

  $idf_i = log\frac{\left | D \right |}{\left | \{j:t_i\in d_j\} \right |}$

  其中，$|D|$语料库中的文件总数。$ |\{j:ti∈dj\}|$ 表示包含词语 ti 的文件数目（即 ni,j≠0 的文件数目）。如果该词语不在语料库中，就会导致分母为零，因此一般情况下使用 $1+|\{j:ti∈dj\}|$

* **TF-IDF实际上是：TF \* IDF**

  某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。

## 2. 核心代码注解

### 1.1 数据读取以及预处理 

+ 先读取数据，为了更好地使用这些数据，我把他转换成了一个矩阵：

  ```python
  df = pd.read_csv(r'test\test_news.tsv',sep = '\t')
  df1 = pd.read_csv(r'test\test.tsv',sep = '\t')
  (m, n) = df.shape
  (m1,n1) = df1.shape
  data_array = np.array(df)
  data_array1 = np.array(df1)
  ```
  
+ 为了更好地计算TF-IDF，需要建立语料库，过程是找出所有新闻的标题，进行分词：

  ```python
  sents = [] #所有的新闻的标题＋简介
  for i in range (m):
      #if (pd.isna(data_array[i][4])):
          sents.append(data_array[i][3])
      #else:
          #sents.append(data_array[i][3]+data_array[i][4])
  sents=[word_tokenize(sent) for sent in sents] #对每个句子进行分词
  print(sents)  #输出分词后的结果
  corpus=TextCollection(sents)  #构建语料库
  print(corpus)  #输出语料库  
  ```
  
  可以看出我把else那一块注释掉了，本来我是想用新闻标题+内容简介拼在一起，完成语料库的建立，后来发现这样程序运行速度过慢，而且标题也是对新闻的概括，于是只添加新闻标题。

* 提取新闻代号，为特定用户计算TF-IDF值的时候可以用

  ```python
  # 按顺序提取所有新闻代号
  all_items_names = np.array(df.iloc[:m+1, 0])
  all_items_names = all_items_names.tolist()
  ```


### 2.2 计算预测值(用户点击的概率) 

这一部分是核心部分，我写了一个大循环，先放上代码，内容在代码后解释：

```python
result_array = []#存放所有结果，即每个用户可能性列表的集合
for i in range (m1):
    user_TF_IDF = {} #存放该用户历史的TF-IDF
    user_items_saw = title(data_array1,i,2)
    user_items_recommed = title(data_array1,i,3) #待推荐的列表
    sum1 = 0 #计算所有新闻总相似度，方便归一化
    probability_list = [] #可能性列表，即最后的结果
    for j in range (len(user_items_saw)):
        items_id = user_items_saw[j]
        index = all_items_names.index(items_id)
        items = sents[index]
        for k in range (len(items)):       
            #计算语料库中该词"word"的tf-idf值
            word = items[k]
            tf_idf=corpus.tf_idf(word,corpus)
            #sum1 += tf_idf
            if (word in user_TF_IDF):
                user_TF_IDF[word] += tf_idf
            else:
                user_TF_IDF[word] = tf_idf
    for m in range (len(user_items_recommed)):
        items_TF_IDF = {} #存放该新闻的TF-IDF
        items_id = user_items_recommed[m] 
        index = all_items_names.index(items_id)
        items = sents[index]
        for n in range (len(items)):       
            #计算语料库中该词"word"的tf-idf值
            word = items[n]
            tf_idf=corpus.tf_idf(word,corpus)
            if (word in items_TF_IDF):
                items_TF_IDF[word] += tf_idf
            else:
                items_TF_IDF[word] = tf_idf
        #计算用户历史和该新闻的相似度即可能性Similarity(A,B)=Σ i∈m TFIDF A∗TFIDF B
        Similarity = 0
        for p in  (items_TF_IDF):
            if (p in user_TF_IDF):
                Similarity += items_TF_IDF[p] * user_TF_IDF[p]
        sum1 += Similarity
        probability_list.append(Similarity)
    for l in range (len(probability_list)): #归一化
        if (sum1 != 0):
            probability_list[l] = probability_list[l]/sum1
    result_array.append(probability_list)
```

* **最外层循环(for i in range)：**这层是对test集每一行进行处理，求出用户喜好，当前页面新闻和用户喜好是否匹配来计算点击的可能性。

* **内层循环1(for j in range)：**这层是对于每一个用户，我求出了该用户历史浏览的新闻的tf-idf集合，这个集合内容类似于{’one‘：0.222,'two':0.0012,'three':0.023.......}是该用户浏览新闻的总结，这个集合可以用于刻画这个用户的喜好。**这一步就类似于我之前说的Item Representation和Profile Learning，用items的特征和用户历史浏览刻画一个用户的喜好。**

* **内层循环2(for m in range)：**这层是进行要推荐的新闻和用户喜好的对比，详细计算过程是

  $Similarity(User,item) = \sum_{i\in m}TFIDF_{User}*TFIDF_{item}$

  $m$是用户喜好和该新闻的共同拥有的词，我们把这种词的TF-IDF值相乘最后相加即得出这个新闻的推荐指数。

* **内层循环3(for l in range):** 这层仅仅是对当前页面所有新闻的点击概率估计的归一化。

### 2.3 写入结果

+ 读取test集并把结果写到result.tsv中，注意原始test.csv的分隔符是'\t'。

  ```python
  data = pd.read_csv(r'test.tsv',sep = '\t')
  data['Predict'] = result_array #将新列的名字设置为cha
  data.to_csv(r"result.tsv",sep = '\t',index =False)
  #mode=a，以追加模式写入,header表示列名，默认为true,index表示行名，默认为true，再次写入不需要行名 
  ```

## 3. 对结果的分析（值得思考的地方）

### 3.1 基于内容的推荐算法的优劣

| 优点                                                         | 缺点                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| 提升推荐结果的相关性 ，因为推荐的内容都是相似的东西          | 无个性化                                                     |
| 结果可解释：如果需要向用户解释为什么推荐了这些产品给他，你只要告诉他这些产品有某某属性，这些属性跟你的品味很匹配等等。 | 依赖于对Item的深度分析： item的特征抽取一般很难              |
| 推荐结果容易被用户感知                                       | 无法挖掘出用户的潜在兴趣                                     |
| 用户之间的独立性：既然每个用户的profile都是依据他本身对item的喜好获得的，自然就与他人的行为无关。这一点和CF完全相反。 | 无法为新用户产生推荐：新用户没有喜好历史，自然无法获得他的profile，所以也就无法为他产生推荐了。当然，这个问题CF也有。 |

### 3.2 用户喜好衡量

实际操作中，有一个问题，用户以前看了辣么多新闻，每个新闻有好些个关键词，我们不太可能把新闻和这些都以一比对。

为了解决这个问题，可以需要引入新的东西：喜好关键词表。

其实很好理解：为每个用户在数据库里维持一个map，这个map里放的都是“用户喜好的关键词-喜好程度”这样的Key-Value对。而这个map最开始当然是空的，而从任意时刻开始，我们可以开始跟踪某用户的浏览行为，每当该用户新浏览了一条新闻，我们就把该新闻的“关键词-TFIDF值”“插入”到该用户的喜好关键词表中。当然这个“插入”要考虑关键词表里已经预先有了某预插入的关键词的情况，那么在这个基础上，我们可以将预插入的关键词的TFIDF值直接和词表里的值加起来。

当然，考虑到存储问题，我们可以为用户的喜好关键词表设定一个容量上限，比如最多1000个词，当然具体数值还是需要在实际运行过程根据效果做调整。

> 这次数据中用户的历史阅读记录比较少，所以我并没有使用喜好关键表，限制容量，上述只是提出的一种 解决方法。

### 3.3 时间复杂度问题

这次实验我写的循环运行时间非常之长，平均10s才能完成对一个用户的预测。思考再三我觉得可以有两个改进的地方，一个是计算TF-IDF的过程，据说有直接的库，连分词都不用做，这个或许会快一点；还有一个是被我付诸于实践的：我想把我想要的、能进行提前运算的数据都先存在不同的文件中，这样运行的时候就可以先读取文件，之间调用数据，但是代码写好后竟然发现文件太大，无法储存......

>  总之这个推荐算法有点慢，不知道怎么能快一些。

### 3.4 兴趣迁移——衰减机制

我想到了，我们的兴趣点可能是会随时间改变的呢？比如这段时间苹果出了一款新产品，我关注一下，但一个月后，我可能就完全不在意这件事了，但是可能苹果相关的关键词还一直在我的关键词表里，那会不会导致我依然收到相似的我已经不关心的新闻的推荐呢？也就是如何处理这种兴趣迁移问题呢？

为了解决这个问题，我们可以引入一个衰减机制，即让用户的关键词表中的每个关键词喜好程度都按一定周期保持衰减。考虑到不同词的TFIDF值可能差异已经在不同的数量级，我们考虑用指数衰减的形式来相对进行公平的衰减。即引入一个$λ$系数，$1>λ>0$，我们每隔一段时间，对所有用户的所有关键词喜好程度进行$*λ$的衰减，那么就完成了模拟用户兴趣迁移的过程。

当然，一直衰减下去，也会使得一些本来就已经完全不感兴趣的关键词可能衰减到了0.0000001了，还在衰减，还死皮赖脸地待在词表里占位置，那么自然而然，我们可以设置一个阈值L，规定对每个用户的每次衰减更新完成后，将词表里喜好值小于L的关键词直接清除。

>  本次实验并没有出现用户点击每个新闻的时间的数据，所以上述只是一个有趣的想法。

## 4.总结

### 4.1 心得

这次实验学到了很多东西，我花了不少时间在研究如何处理数据上，pandas使用更加熟练。初步实现了一个简单的入门级智能推荐系统算法（CB），尽管代码中还有很多可以加强的地方，但是仍然使我对智能推荐系统有了更深刻的理解。

### 4.2 作业上交内容

+ 实验报告

  > 10185102144.pdf

+ 可运行的代码

  > CB_基于TFIDF的新闻推荐.ipynb
  >
  > 放在了code文件夹里
  >
  > 运行一次大概需要2~3个小时，不过确实是可运行的

+ 测试集

  > test_news.csv、test.csv为了确保提交的代码可运行
  >
  > 放在了code文件夹里

+ 最终结果

  > test.tsv

### 4.3 参考文献

https://blog.csdn.net/qq_32690999/article/details/77434381

> TF-IDF和新闻推荐的结合

https://blog.csdn.net/asialee_bird/article/details/81486700

> TF-IDF算法实现

https://www.jb51.net/article/166731.htm

> 对文件的处理

